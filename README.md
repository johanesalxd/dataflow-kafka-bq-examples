# Dataflow Kafka to BigQuery Examples

This repository contains examples for streaming data from Kafka to BigQuery using both custom Java pipelines and Google's pre-built Dataflow templates.

## Project Structure

```
.
â”œâ”€â”€ pom.xml                                 # Maven project configuration
â”œâ”€â”€ run_local.sh                            # Simple local pipeline (Branch 1 only)
â”œâ”€â”€ run_dataflow.sh                         # Simple Dataflow pipeline (Branch 1 only)
â”œâ”€â”€ run_branched_pipeline.sh                # Advanced pipeline with all three branches
â”œâ”€â”€ run_multi_pipeline.sh                   # Multi-stream join pipeline (NEW)
â”œâ”€â”€ run_dataflow_template.sh                # Script to deploy using Dataflow Flex Template
â”œâ”€â”€ MULTI_PIPELINE_README.md                # Documentation for multi-stream joins
â”œâ”€â”€ udf/
â”‚   â”œâ”€â”€ process.js                          # JavaScript UDF for template transformation
â”‚   â””â”€â”€ user_event_aggregations.sql         # SQL query for Beam SQL aggregations
â”œâ”€â”€ src/main/java/com/johanesalxd/          # Java source code
â”‚   â”œâ”€â”€ KafkaToBigQuery.java                # Main pipeline logic with three branches
â”‚   â”œâ”€â”€ MultiPipelineExample.java           # Multi-stream join pipeline (NEW)
â”‚   â”œâ”€â”€ KafkaPipelineOptions.java           # Pipeline options for single-topic
â”‚   â”œâ”€â”€ MultiPipelineOptions.java           # Pipeline options for multi-stream (NEW)
â”‚   â”œâ”€â”€ schemas/                            # BigQuery schema definitions
â”‚   â”‚   â”œâ”€â”€ UserEventBigQuerySchema.java
â”‚   â”‚   â”œâ”€â”€ GenericBigQuerySchema.java
â”‚   â”‚   â”œâ”€â”€ UserEventAggregationSchema.java
â”‚   â”‚   â”œâ”€â”€ UserProfileBigQuerySchema.java  # NEW
â”‚   â”‚   â”œâ”€â”€ ProductUpdateBigQuerySchema.java # NEW
â”‚   â”‚   â””â”€â”€ EnrichedEventBigQuerySchema.java # NEW
â”‚   â””â”€â”€ transforms/                         # Data transformation classes
â”‚       â”œâ”€â”€ JsonToTableRow.java
â”‚       â”œâ”€â”€ JsonToGenericTableRow.java
â”‚       â”œâ”€â”€ JsonToRow.java
â”‚       â”œâ”€â”€ UserProfileToTableRow.java      # NEW
â”‚       â”œâ”€â”€ UserProfileToRow.java           # NEW
â”‚       â”œâ”€â”€ ProductUpdateToRow.java         # NEW
â”‚       â”œâ”€â”€ EnrichedEventToTableRow.java    # NEW
â”‚       â”œâ”€â”€ TableRowToUserProfileRow.java   # NEW
â”‚       â”œâ”€â”€ EnrichWithUserProfileDoFn.java  # NEW
â”‚       â””â”€â”€ SqlQueryReader.java
â”œâ”€â”€ src/main/resources/udf/                 # SQL queries for Beam SQL
â”‚   â”œâ”€â”€ enriched_events_join.sql            # Multi-stream join query (NEW)
â”‚   â””â”€â”€ user_event_aggregations.sql         # Event aggregation query
â”œâ”€â”€ kafka-tools/                            # Kafka producer/consumer tools
â”‚   â”œâ”€â”€ data_generator.py
â”‚   â”œâ”€â”€ data_consumer.py
â”‚   â”œâ”€â”€ detailed_consumer_groups.py
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ example_usage.md
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ vm-deployment/                      # VM deployment scripts
â”‚       â”œâ”€â”€ 1-provision-kafka-vm.sh
â”‚       â”œâ”€â”€ 2-setup-kafka-vm.sh
â”‚       â”œâ”€â”€ docker-compose-vm.yml
â”‚       â””â”€â”€ example_usage.md
â””â”€â”€ schemas/                                # JSON schemas for BigQuery
    â”œâ”€â”€ user_events.json
    â”œâ”€â”€ user_profiles.json                  # NEW
    â”œâ”€â”€ product_updates.json                # NEW
    â”œâ”€â”€ enriched_events.json                # NEW
    â”œâ”€â”€ generic_table.json
    â””â”€â”€ user_event_aggregations.json
```

## Pipeline Scripts Overview

This repository provides multiple deployment scripts for different use cases:

### Simple Pipelines (Single Topic â†’ Single Table)
- **`run_local.sh`** - Local development with DirectRunner (Branch 1 only)
- **`run_dataflow.sh`** - Simple Dataflow deployment (Branch 1 only)

### Advanced Pipelines
- **`run_branched_pipeline.sh`** - Single topic with 3 parallel processing branches
- **`run_multi_pipeline.sh`** - Multi-stream joins with Beam SQL (NEW)
- **`run_dataflow_template.sh`** - Google's managed Flex Template

### Branch Behavior in KafkaToBigQuery.java

The `KafkaToBigQuery.java` pipeline has three conditional branches:

| Branch | Trigger | Purpose | Output |
|--------|---------|---------|---------|
| **Branch 1** | Always runs (requires `outputTable`) | Flattened structured data | `raw_user_events` |
| **Branch 2** | Only if `genericOutputTable` provided | Raw JSON with timestamps | `raw_user_events_flex` |
| **Branch 3** | Only if `sqlAggregationTable` provided | Real-time SQL aggregations | `user_event_aggregations` |

- **Simple scripts** (`run_local.sh`, `run_dataflow.sh`) only provide `outputTable` â†’ Branch 1 only
- **Branched script** (`run_branched_pipeline.sh`) provides all three tables â†’ All branches run
- **Multi-pipeline script** (`run_multi_pipeline.sh`) runs completely different pipeline (`MultiPipelineExample.java`)

### âš ï¸ Critical: Main Class Execution

**IMPORTANT**: All scripts use explicit main class specification since no main class is defined in the JAR manifest:

```bash
# âœ… CORRECT - Only method that works
java -cp ${JAR_FILE} com.johanesalxd.KafkaToBigQuery

# âŒ FAILS - No main class in JAR manifest
java -jar ${JAR_FILE}
java -jar ${JAR_FILE} com.johanesalxd.KafkaToBigQuery  # Still fails!
```

**Why this approach:**
- The JAR manifest has **no main class** defined (removed from pom.xml for safety)
- Using `java -jar` will **always fail** with "no main manifest attribute" error
- Only `java -cp` with explicit main class works
- This prevents accidentally running the wrong pipeline

**Benefits:**
- **Explicit Control**: Must specify which main class to run
- **No Ambiguity**: Cannot accidentally run the wrong pipeline
- **Consistency**: All scripts use the same execution method (`java -cp`)
- **Safety**: Impossible to run without explicitly choosing the pipeline

## Quick Start

This repository provides multiple approaches for streaming Kafka data to BigQuery:

### Option 1: Custom Java Pipeline (Local Development)

1.  **Start Kafka:**
    ```bash
    docker-compose up -d
    ```

2.  **Install Java and Maven:**
    This project requires Java 11 and Maven to be installed on your system.

3.  **Generate Sample Data:**
    In a separate terminal, run the data generator to start producing messages to the `user-events` topic:
    ```bash
    python3 kafka-tools/data_generator.py --topics user-events
    ```

4.  **Run the Pipeline Locally:**
    In another terminal, run the following command to start the Beam pipeline locally:
    ```bash
    ./run_local.sh
    ```

The pipeline will start consuming messages from the `user-events` topic and writing them to the `raw_user_events` table in the `dataflow_demo_local` dataset in BigQuery.

### Option 2: Dataflow Flex Template (Production Ready)

For a simpler deployment without Java/Maven requirements:

1.  **Start Kafka:** (same as above)
2.  **Generate Sample Data:** (same as above)
3.  **Configure and Deploy:**
    ```bash
    # Edit run_dataflow_template.sh with your GCP settings
    ./run_dataflow_template.sh
    ```

This approach uses Google's managed template and writes to the `raw_user_events_flex` table.

### Option 3: Multi-Branch Pipeline with Beam SQL (Advanced)

For demonstrating multiple processing approaches including real-time SQL aggregations:

1.  **Start Kafka:** (same as above)
2.  **Generate Sample Data:** (same as above)
3.  **Deploy Multi-Branch Pipeline:**
    ```bash
    # Edit run_branched_pipeline.sh with your GCP settings
    ./run_branched_pipeline.sh
    ```

This pipeline processes the same Kafka stream into **three parallel branches**:
- **Flattened events** â†’ `raw_user_events` (structured schema)
- **Generic events** â†’ `raw_user_events_flex` (raw JSON with timestamps)
- **SQL aggregations** â†’ `user_event_aggregations` (real-time event counts by type)

The SQL aggregation branch demonstrates **Beam SQL** capabilities with 1-minute tumbling windows, counting events by type using the query in `udf/user_event_aggregations.sql`.

### Option 4: Multi-Stream Join Pipeline (NEW)

For demonstrating advanced stream processing with multiple Kafka topics and Beam SQL joins:

```bash
# Edit run_multi_pipeline.sh with your GCP settings
./run_multi_pipeline.sh
```

This pipeline demonstrates advanced multi-stream joins with BigQuery side inputs and Beam SQL.

ğŸ“– **For detailed documentation, see [MULTI_PIPELINE_README.md](MULTI_PIPELINE_README.md)**

## Deploying to Google Cloud Dataflow

For production deployments, you can run the same pipeline on Google Cloud Dataflow:

For a simpler deployment without Java/Maven requirements:

1.  **Start Kafka:** (same as above)
2.  **Generate Sample Data:** (same as above)
3.  **Configure and Deploy:**
    ```bash
    # Edit run_dataflow_template.sh with your GCP settings
    ./run_dataflow.sh
    ```

This approach uses Google's managed template and writes to the `raw_user_events` table.

### Monitoring
- View jobs: [Dataflow Console](https://console.cloud.google.com/dataflow)
- Monitor data: [BigQuery Console](https://console.cloud.google.com/bigquery)

## Using Dataflow Flex Template (Alternative Approach)

For users who prefer to use Google's pre-built templates instead of custom Java code, this repository also includes a template-based approach using the `Kafka_to_BigQuery_Flex` template.

### Template vs Custom Pipeline Comparison

| Feature | Custom Java Pipeline | Dataflow Flex Template |
|---------|---------------------|------------------------|
| **Setup** | Requires Java 11 + Maven | Only requires gcloud CLI |
| **Deployment** | Compile + Deploy | Direct deployment |
| **Customization** | Full Java flexibility | JavaScript UDF only |
| **Maintenance** | User maintains code | Google maintains template |
| **Table Schema** | Custom schema | Generic schema with payload |
| **Output Table** | `raw_user_events` | `raw_user_events_flex` |

## Features

*   **Java-based:** A reliable and robust implementation using the Java SDK.
*   **Multi-Branch Processing:** Single pipeline with three parallel processing branches for different use cases.
*   **Beam SQL Integration:** Real-time SQL aggregations with external query files for easy maintenance.
*   **Simplified Configuration:** All configuration is handled in the deployment scripts, making it easy to get started.
*   **Partitioned BigQuery Tables:** All pipelines write to time-partitioned BigQuery tables, which is a best practice for managing large datasets.
*   **Clean and Simple Code:** The code is well-structured and easy to understand, making it a great starting point for more complex pipelines.
*   **Flexible Schema Management:** Support for both structured and generic schemas with automatic table creation.
